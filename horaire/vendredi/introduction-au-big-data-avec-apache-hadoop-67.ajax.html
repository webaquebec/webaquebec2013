<figure>
    <img src="/assets/images/speakers/400x375/jean-daniel_cryans-xlarge.jpg" width="400" height="375" alt="Jean-Daniel Cryans" />
    <figcaption>
      <h1>
      Jean-Daniel Cryans
            <a class="twitter-bird" href="http://www.twitter.com/jdcryans" title="Jean-Daniel Cryans sur twitter" target="_blank"><span class="shy">twitter</span></a>
            </h1>
    </figcaption>
</figure>
<div class="content" data-shareURL="http://webaquebec2013.herokuapp.com/horaire/vendredi/introduction-au-big-data-avec-apache-hadoop-67">
    <h2>Introduction au Big Data avec Apache Hadoop</h2>
    <span class="time"><time itemprop="startDate" datetime="2013-02-22T13:15:00">vendredi le 22 février, de 13 h 15</time> à <time itemprop="endDate" datetime="2013-02-22T14:00:00">14 h 00</time></span>
    <span class="room">Salle Deloitte</span>
    <div id="scrollbar1" class="desc-wrap">
        <div class="scrollbar"><div class="track"><div class="thumb"><div class="end"></div></div></div></div>
        <div class="viewport">
            <div class="overview">
                                    <p>Les entreprises d'aujourd'hui produisent plus de données qu'elles ne sont capables d'en traiter. Pensez aux logs qui proviennent des applications web comme Apache, il serait impensable de vouloir les entreposer dans une base de données conventionnelle puisque ce serait trop cher du teraoctet. C'est le type de problème qui représente bien le "Big Data" et les outils de ce domaine qui ont pour but de rendre l'entreposage et le traitement de ces données plus rentable ou même profitable.</p>

<p>Un de ces outils est Apache Hadoop. Il consiste en un système de fichiers distribués et une implementation du cadre programmatique MapReduce (inventé par Google). Il devient maintenant possible, voir facile, d'entreposer et de traiter des teraoctets de données sans avoir recours à des serveurs spécialisés tout en utilisant du code qui peut être distribué sur 3 ou 1000 machines sans être modifié.</p>

<p>Cette présentation fera une introduction à ces concepts et poursuivra avec la présentation de cas d'utilisations spécifiques au domaine du web.</p>
                              
                                    <h2>Les trois principales questions auxquelles la présentation répondra</h2>
                    <ol>
    <li>Qu'est-ce que le Big Data?</li>
    <li>Qu'est-ce que Hadoop?</li>
    <li>Est-ce que ça peut servir mon entreprise?</li>
</ol>
                              
                                                            <h2>Biographie</h2>
                        <p> Jean-Daniel Cryans occupe le poste d'ingénieur en logiciel à Cloudera depuis octobre 2012. Il travaille avec l'équipe de Storage où il aide à développer Apache HBase. Précédemment, il travaillait à StumbleUpon en tant qu’ingénieur en base de données où il participait au développement de HBase en plus de maintenir les grappes de centaines de serveurs Hadoop et HBase en production. Jean-Daniel est devenu “committer” et membre du comité de gestion du projet (PMC) Apache HBase en 2008 alors qu’il était encore étudiant à l’École de technologie supérieure (ÉTS) à Montréal. Il habite aujourd’hui avec sa femme à San Francisco. </p>
                                                    
              <div class="bio-meta">
                                    <span><strong>Entreprise</strong>: Cloudera</span>
                                  
                                    <span><strong>Titre</strong>: Software Engineer</span>
                                
                                </div>
              
            </div>
        </div>
    </div>
    
</div>
